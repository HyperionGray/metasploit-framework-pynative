"""Replace dead reference links with archived snapshots.

This script reads the JSON output produced by
``tools/dev/detect_dead_reference_links.py`` (or the Ruby equivalent) and
replaces URLs inside repository files with the archived snapshot URLs found in
that JSON file.

Usage:
    python3 tools/dev/find_and_replace_dead_reference_links.py -f url_check_results.json

The JSON file is expected to contain a list of objects with the following keys:
``url`` (the URL that was checked), ``path`` (the relative path to the file
containing the URL), and ``archived_snapshot`` (the replacement URL).  Entries
with missing snapshots are skipped automatically.
"""

from __future__ import annotations

import argparse
import json
import os
from pathlib import Path
from typing import Iterable, List, Mapping


def load_json(file_path: Path) -> List[Mapping[str, str]]:
    """Load JSON data from *file_path*.

    Args:
        file_path: The path to the JSON file that should be loaded.

    Returns:
        The parsed JSON data.

    Raises:
        FileNotFoundError: If the JSON file does not exist.
        json.JSONDecodeError: If the JSON file contains invalid JSON.
    """

    with file_path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def replace_links_in_files(data: Iterable[Mapping[str, str]]) -> None:
    """Replace URLs in files using archived snapshots.

    Args:
        data: An iterable of dictionaries that include the keys ``url``,
            ``path``, and ``archived_snapshot``.
    """

    cwd = Path(os.getcwd())

    for index, entry in enumerate(data, start=1):
        original_url = entry.get("url", "")
        archived_snapshot = entry.get("archived_snapshot")
        path = entry.get("path")

        print(
            f"Processing entry {index}: {original_url} -> {archived_snapshot}",
        )

        if path is None:
            print(f"Skipping entry {index} because no file path was provided.")
            continue

        # Remove the leading 'URL-' prefix that the detector adds.
        url = original_url[4:] if original_url.startswith("URL-") else original_url

        # Skip entries without an archived version or when an error occurred.
        if not archived_snapshot or archived_snapshot.startswith("Error fetching Wayback"):
            print(
                "Skipping entry {index} because no archived version is available "
                "or there was an error fetching it.".format(index=index)
            )
            continue

        full_path = cwd / path

        if not full_path.exists():
            print(f"File not found: {full_path}")
            continue

        try:
            file_content = full_path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            print(f"Unable to read file (encoding issue): {full_path}")
            continue

        updated_content = file_content.replace(url, archived_snapshot)

        if file_content == updated_content:
            print(f"No change needed for file: {full_path}")
            continue

        full_path.write_text(updated_content, encoding="utf-8")
        print(f"Replaced URL in file: {full_path}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "-f",
        "--file",
        default="url_check_results.json",
        type=Path,
        help="Path to the JSON file generated by detect_dead_reference_links.",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    json_data = load_json(args.file)
    replace_links_in_files(json_data)
if __name__ == "__main__":
    main()
