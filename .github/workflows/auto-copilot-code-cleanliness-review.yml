name: "Periodic Code Cleanliness Review"

# REQUIREMENTS:
# - A GitHub Personal Access Token with Copilot access must be created and stored as a repository secret named COPILOT_TOKEN
# - See COPILOT_TOKEN_SETUP.md for detailed setup instructions

on:
  schedule:
    # Run every 12 hours (at 00:00 and 12:00 UTC)
    - cron: '0 0,12 * * *'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: write
  pull-requests: write
  issues: write

jobs:
  code-cleanliness-review:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@main
        with:
          fetch-depth: 0  # Full history for better analysis

      - name: Analyze Large Files
        id: analyze
        run: |
          echo "## Large Files Analysis" > /tmp/analysis.md
          echo "" >> /tmp/analysis.md
          echo "Files larger than 500 lines that may benefit from splitting:" >> /tmp/analysis.md
          echo "" >> /tmp/analysis.md
          
          # Find files larger than 500 lines (excluding common large files)
          find . -type f \( -name "*.py" -o -name "*.js" -o -name "*.ts" -o -name "*.java" -o -name "*.go" -o -name "*.cs" -o -name "*.rb" \) \
            ! -path "*/node_modules/*" \
            ! -path "*/dist/*" \
            ! -path "*/build/*" \
            ! -path "*/.venv/*" \
            ! -path "*/vendor/*" \
            -exec wc -l {} \; | \
            awk '$1 > 500 {print $1 " lines: " $2}' | \
            sort -rn >> /tmp/analysis.md || echo "No large files found" >> /tmp/analysis.md
          
          echo "" >> /tmp/analysis.md
          echo "## Code Complexity Analysis" >> /tmp/analysis.md
          echo "" >> /tmp/analysis.md
          echo "Files with potential complexity issues:" >> /tmp/analysis.md
          
          # Find files with many functions/classes (basic heuristic)
          for ext in py js ts java go cs rb; do
            if [ "$ext" = "py" ]; then
              pattern="^def |^class "
            elif [ "$ext" = "js" ] || [ "$ext" = "ts" ]; then
              pattern="^function |^class |const.*=.*=>|function.*{$"
            else
              pattern="^class |^def |^func "
            fi
            
            find . -type f -name "*.$ext" \
              ! -path "*/node_modules/*" \
              ! -path "*/dist/*" \
              ! -path "*/build/*" \
              ! -path "*/.venv/*" \
              ! -path "*/vendor/*" \
              -exec sh -c 'count=$(grep -c "$1" "$2" 2>/dev/null || echo 0); if [ "$count" -gt 20 ]; then echo "$count definitions in $2"; fi' _ "$pattern" {} \; \
              2>/dev/null || true
          done | sort -rn >> /tmp/analysis.md
          
          cat /tmp/analysis.md

      - name: Code Cleanliness Review
        run: |
          echo "ðŸ§¹ Performing comprehensive code cleanliness review..."
          echo "===================================================="
          
          # 1. Large Files Analysis
          echo "ðŸ“ Large Files Analysis (>500 lines)"
          echo "------------------------------------"
          
          echo "Files larger than 500 lines:"
          find . -name "*.py" -type f -exec wc -l {} + | awk '$1 > 500 {print $1 " lines: " $2}' | sort -nr | head -20
          
          echo ""
          echo "Recommendations for large files:"
          echo "- Consider splitting files >1000 lines into smaller modules"
          echo "- Extract utility functions into separate helper modules"
          echo "- Use composition over inheritance for complex classes"
          
          # 2. Code Duplication Analysis
          echo ""
          echo "ðŸ”„ Code Duplication Analysis"
          echo "----------------------------"
          
          # Simple duplication check using common patterns
          echo "Checking for potential code duplication..."
          
          # Find similar function signatures
          echo "Similar function definitions:"
          grep -r "^def " python_framework/ lib/ --include="*.py" | cut -d: -f2 | sort | uniq -c | sort -nr | head -10 | awk '$1 > 1 {print "Potential duplicate: " $0}'
          
          # Find similar import patterns
          echo ""
          echo "Common import patterns (potential for consolidation):"
          grep -r "^import\|^from" python_framework/ lib/ --include="*.py" | cut -d: -f2 | sort | uniq -c | sort -nr | head -10
          
          # 3. Code Style and Formatting
          echo ""
          echo "ðŸŽ¨ Code Style and Formatting"
          echo "-----------------------------"
          
          # Check for consistent indentation
          echo "Checking indentation consistency..."
          mixed_indent=$(find python_framework/ lib/ -name "*.py" -exec grep -l $'\t' {} \; 2>/dev/null | wc -l)
          echo "Files with mixed indentation (tabs found): $mixed_indent"
          
          # Check line length
          echo "Checking line length (>120 characters):"
          long_lines=$(find python_framework/ lib/ -name "*.py" -exec awk 'length > 120 {count++} END {print count+0}' {} + | awk '{sum += $1} END {print sum+0}')
          echo "Lines longer than 120 characters: $long_lines"
          
          # Run flake8 if available
          if command -v flake8 &> /dev/null; then
            echo ""
            echo "Running flake8 style checker..."
            flake8 python_framework/ lib/ --count --max-line-length=120 --statistics || true
          fi
          
          # 4. Complex Functions Analysis
          echo ""
          echo "ðŸ§  Complex Functions Analysis"
          echo "-----------------------------"
          
          echo "Functions with high complexity (>50 lines):"
          python3 -c "
          import ast
          import os
          
          def analyze_complexity(filepath):
              try:
                  with open(filepath, 'r') as f:
                      tree = ast.parse(f.read())
                  
                  complex_functions = []
                  for node in ast.walk(tree):
                      if isinstance(node, ast.FunctionDef):
                          # Estimate complexity by counting lines
                          if hasattr(node, 'end_lineno') and node.end_lineno:
                              lines = node.end_lineno - node.lineno
                              if lines > 50:
                                  complex_functions.append((filepath, node.name, node.lineno, lines))
                  return complex_functions
              except:
                  return []
          
          for root, dirs, files in os.walk('python_framework'):
              for file in files:
                  if file.endswith('.py'):
                      filepath = os.path.join(root, file)
                      complex_funcs = analyze_complexity(filepath)
                      for func_info in complex_funcs[:5]:  # Limit output
                          print(f'{func_info[0]}:{func_info[2]} - {func_info[1]} ({func_info[3]} lines)')
          " || true
          
          # 5. Code Organization Analysis
          echo ""
          echo "ðŸ“ Code Organization Analysis"
          echo "-----------------------------"
          
          # Check for proper module structure
          echo "Module structure analysis:"
          echo "Python packages (directories with __init__.py):"
          find python_framework/ lib/ -name "__init__.py" | wc -l
          
          echo ""
          echo "Modules without __init__.py (potential organization issue):"
          find python_framework/ lib/ -type d -not -path "*/.*" -not -path "*/__pycache__" | while read dir; do
            if [ ! -f "$dir/__init__.py" ] && [ "$(find "$dir" -maxdepth 1 -name "*.py" | wc -l)" -gt 0 ]; then
              echo "  $dir"
            fi
          done | head -10
          
          # 6. Separation of Concerns
          echo ""
          echo "ðŸŽ¯ Separation of Concerns Analysis"
          echo "----------------------------------"
          
          echo "Files mixing multiple concerns (containing both classes and functions):"
          python3 -c "
          import ast
          import os
          
          def check_separation(filepath):
              try:
                  with open(filepath, 'r') as f:
                      tree = ast.parse(f.read())
                  
                  has_classes = False
                  has_functions = False
                  
                  for node in ast.walk(tree):
                      if isinstance(node, ast.ClassDef):
                          has_classes = True
                      elif isinstance(node, ast.FunctionDef) and not any(isinstance(parent, ast.ClassDef) for parent in ast.walk(tree) if hasattr(parent, 'body') and node in getattr(parent, 'body', [])):
                          has_functions = True
                  
                  return has_classes and has_functions
              except:
                  return False
          
          mixed_files = []
          for root, dirs, files in os.walk('python_framework'):
              for file in files:
                  if file.endswith('.py') and not file.startswith('__'):
                      filepath = os.path.join(root, file)
                      if check_separation(filepath):
                          mixed_files.append(filepath)
          
          for file in mixed_files[:10]:  # Limit output
              print(f'Mixed concerns: {file}')
          " || true
          
          echo ""
          echo "ðŸ“‹ Code Cleanliness Recommendations:"
          echo "-----------------------------------"
          echo "1. Split large files (>1000 lines) into focused modules"
          echo "2. Extract common functionality into utility modules"
          echo "3. Use consistent code formatting (consider using black)"
          echo "4. Break down complex functions into smaller, focused functions"
          echo "5. Ensure proper module organization with __init__.py files"
          echo "6. Separate classes and utility functions into different modules"
          echo "7. Use type hints for better code documentation"
          echo "8. Consider using dataclasses for simple data containers"
          
          echo ""
          echo "âœ… Code cleanliness review complete."
        continue-on-error: true

      - name: Create Issue for Code Cleanliness Review
        uses: actions/github-script@main
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const analysis = fs.readFileSync('/tmp/analysis.md', 'utf8');
            
            const date = new Date().toISOString().split('T')[0];
            const title = `Code Cleanliness Review - ${date}`;
            
            const body = `# Periodic Code Cleanliness Review
            
            This is an automated review conducted every 12 hours to maintain code quality.
            
            ${analysis}
            
            ## Recommendations
            
            Please review the analysis above and:
            1. Split large files (>500 lines) into smaller, focused modules
            2. Refactor complex functions into smaller, testable units
            3. Remove code duplication
            4. Ensure consistent code style
            5. Improve code organization and structure
            
            ## Next Steps
            
            - Assign this issue to relevant team members
            - Create follow-up PRs to address findings
            - Document any architectural decisions
            
            ---
            *This issue was automatically generated by the Code Cleanliness Review workflow.*
            `;
            
            // Check if similar issue exists (open, created in last 24 hours)
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: ['code-cleanliness', 'automated'],
              per_page: 10
            });
            
            const recentIssue = issues.data.find(issue => {
              const createdAt = new Date(issue.created_at);
              const hoursSinceCreation = (Date.now() - createdAt) / (1000 * 60 * 60);
              return hoursSinceCreation < 24;
            });
            
            if (recentIssue) {
              console.log(`Recent issue found: #${recentIssue.number}, skipping creation`);
              // Update existing issue with new analysis
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: recentIssue.number,
                body: `## Updated Analysis (${date})\n\n${analysis}`
              });
            } else {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['code-cleanliness', 'automated', 'needs-review']
              });
            }
